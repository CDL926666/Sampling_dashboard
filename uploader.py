#!/usr/bin/env python3
# ======================================================================
#  uploader.py Â· v1.9.1  (ä¼˜åŒ–è¿›ç¨‹ç®¡ç†ã€æ—¥å¿—flushåŠå‰ç«¯åˆ·æ–°è°ƒç”¨)
# ======================================================================
from __future__ import annotations
import io, sys, shutil, tempfile, subprocess, textwrap, gzip, pathlib, os, re, time, json, atexit
from datetime import datetime

import pandas as pd
import streamlit as st
from filelock import FileLock, Timeout

# â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MAX_PART_MB  = 200
COMPRESS_OUT = False

RAW_ROOT     = pathlib.Path("user_uploads")
FINAL_PATH   = pathlib.Path("final/output_ch4_flux_qc.csv")
BACKEND_CMD  = [sys.executable, "PY3/run_all.py"]
LOCK_PATH    = pathlib.Path("sampling_engine/run_all.lock")

# è¿›åº¦ / æ—¥å¿— / PID æ–‡ä»¶ï¼ˆä¸ run_all.py / app.py ä¿æŒä¸€è‡´ï¼‰
MON_DIR   = pathlib.Path("sampling_engine")
PROGRESS  = MON_DIR / "progress.json"
LOG_PATH  = MON_DIR / "progress.log"
PID_PATH  = MON_DIR / "pid.txt"

for p in (RAW_ROOT, LOCK_PATH.parent, MON_DIR):
    p.mkdir(parents=True, exist_ok=True)

_log_re = re.compile(r"(error|fail|exception|traceback|warning)", re.I)
_mb = lambda b: round(b / 2**20, 2)

# å®‰å…¨æ¸…ç†PIDæ–‡ä»¶ï¼Œé˜²æ­¢æ®‹ç•™
def cleanup_pid():
    try:
        PID_PATH.unlink(missing_ok=True)
    except Exception:
        pass
atexit.register(cleanup_pid)  # ç¨‹åºé€€å‡ºè‡ªåŠ¨æ¸…ç†

# â”€â”€ å·¥å…·å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _detect_enc(p: pathlib.Path) -> str:
    try:
        import magic
        mime = magic.from_file(str(p), mime=True)
        if "charset=" in mime:
            ch = mime.split("charset=")[-1].lower()
            return "gbk" if ch.startswith("gb") else "utf-8"
    except ImportError:
        pass
    return "utf-8"

def _merge(parts: list[pathlib.Path], dst: pathlib.Path) -> pathlib.Path:
    tot = sum(p.stat().st_size for p in parts)
    done, bar = 0, st.progress(0.0, text="Merging â€¦")
    if COMPRESS_OUT and dst.suffix != ".gz":
        dst = dst.with_suffix(dst.suffix + ".gz")
        opener = lambda p: gzip.open(p, "wt", encoding="utf-8")
    else:
        opener = lambda p: open(p, "w", encoding="utf-8")

    with opener(dst) as fout:
        for i, part in enumerate(parts):
            with open(part, encoding=_detect_enc(part), errors="ignore") as fin:
                if i: next(fin)                         # è·³è¿‡é¢å¤–è¡¨å¤´
                for chunk in iter(lambda: fin.read(1 << 20), ""):
                    if not chunk:
                        break
                    fout.write(chunk)
                    done += len(chunk.encode())
                    bar.progress(done / tot,
                                 text=f"Merging â€¦ {_mb(done)}/{_mb(tot)} MB")
    bar.empty()
    return dst

# â”€â”€ è¿›ç¨‹æ§åˆ¶æŒ‰é’®åŒºåŸŸï¼Œå•ç‹¬å°è£…ï¼Œç¡®ä¿ä»»ä½•çŠ¶æ€ä¸‹éƒ½æ˜¾ç¤º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _render_stop_button():
    st.markdown("---")
    st.caption("ğŸ’» è¿›ç¨‹æ§åˆ¶")
    if PID_PATH.exists():
        try:
            pid = PID_PATH.read_text().strip()
            st.warning(f"åç«¯è¿è¡Œä¸­ï¼ŒPID={pid}")
            if st.button("â›” å¼ºåˆ¶ç»ˆæ­¢åå°ä»»åŠ¡"):
                try:
                    os.kill(int(pid), 9)
                    cleanup_pid()
                    # æ¸…ç†è¿›åº¦çŠ¶æ€ï¼Œé˜²æ­¢å‰ç«¯è¯¯åˆ¤åå°ä»åœ¨è¿è¡Œ
                    PROGRESS.write_text(json.dumps({"stage": "FINISHED", "ts": time.time()}))
                    st.success("å·²å°è¯•ç»ˆæ­¢åå°ä»»åŠ¡ã€‚è¯·åˆ·æ–°é¡µé¢ã€‚")
                except Exception as e:
                    st.error(f"ç»ˆæ­¢å¤±è´¥ï¼š{e}")
        except Exception as e:
            st.error(f"è¯»å– PID æ–‡ä»¶å‡ºé”™: {e}")
    else:
        st.success("åç«¯ç©ºé—²ï¼Œå¯æ–°ä»»åŠ¡ä¸Šä¼ ã€‚")

# â”€â”€ UI ä¸»å…¥å£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def render() -> None:
    st.subheader("ğŸ“¤ ä¸Šä¼  CHâ‚„ CSVï¼ˆâ‰¤ 200 MB / partï¼‰")

    # æ£€æŸ¥åå°è¿è¡ŒçŠ¶æ€ï¼Œä¸é˜»æ–­ä¸Šä¼ ç•Œé¢ï¼Œåªç¦ç”¨ä¸Šä¼ æŒ‰é’®
    running_stage = None
    is_running = False
    if PROGRESS.exists():
        try:
            info = json.loads(PROGRESS.read_text())
            running_stage = info.get("stage")
            is_running = running_stage not in ("FINISHED", "starting")
        except Exception:
            pass

    if is_running:
        st.info(f"åç«¯æ­£åœ¨è¿è¡Œï¼š{running_stage}ï¼Œä¸Šä¼ æŒ‰é’®å·²ç¦ç”¨ï¼Œè¯·ç¨å€™ã€‚")

    files = st.file_uploader(
        "ä¸€æ¬¡é€‰å®Œæ‰€æœ‰åˆ†ç‰‡ï¼›è‹¥å•æ–‡ä»¶ï¼200 MBï¼Œè¯·å…ˆæœ¬åœ°åˆ‡åˆ†",
        type="csv",
        accept_multiple_files=True,
        key="uploader",
        disabled=is_running,
    )

    # æ— è®ºåå°æ˜¯å¦è¿è¡Œï¼Œéƒ½æ˜¾ç¤ºåœæ­¢æŒ‰é’®ï¼Œæ–¹ä¾¿å¼ºåˆ¶åœæ­¢
    _render_stop_button()

    if not files:
        st.info("ç­‰å¾…æ–‡ä»¶ â€¦")
        return

    # ---------- ä¿å­˜ä¸Šä¼ ä¸´æ—¶æ–‡ä»¶ ----------
    tmpdir = pathlib.Path(tempfile.mkdtemp(prefix="csv_parts_"))
    parts: list[pathlib.Path] = []
    oversize = False
    with st.status("ä¿å­˜ä¸Šä¼ æ–‡ä»¶ â€¦", expanded=False) as stat:
        for uf in files:
            buf = uf.getbuffer()
            mb = _mb(len(buf))
            stat.write(f"â€¢ {uf.name} {mb:.2f} MB")
            if mb > MAX_PART_MB:
                oversize = True
                stat.write(f"  âŒ è¶…å‡º {MAX_PART_MB} MB é™åˆ¶")
            else:
                p = tmpdir / uf.name
                p.write_bytes(buf)
                parts.append(p)
        stat.update(label="ä¸Šä¼ å®Œæ¯• âœ“" if not oversize else "ä¸Šä¼ ä¸­æ–­",
                    state="complete" if not oversize else "error")
    if oversize:
        st.error(f"æœ‰æ–‡ä»¶è¶…è¿‡ {MAX_PART_MB} MBï¼Œè¯·åˆ‡åˆ†åå†ä¸Šä¼ ã€‚")
        return

    parts.sort()
    st.success(f"å…± {len(parts)} åˆ†ç‰‡ï¼Œå…¨éƒ¨ â‰¤ {MAX_PART_MB} MB")

    # ---------- è¡¨å¤´ä¸€è‡´æ€§æ£€æŸ¥ ----------
    hdrs = [open(p, encoding=_detect_enc(p), errors="ignore").readline().strip()
            for p in parts]
    if len(set(hdrs)) != 1:
        st.error("âŒ åˆ†ç‰‡è¡¨å¤´ä¸ä¸€è‡´ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶ã€‚")
        return
    st.success("Header ä¸€è‡´æ€§é€šè¿‡")

    # ---------- é¢„è§ˆ ----------
    if st.toggle("é¢„è§ˆåˆå¹¶åå‰ 5 è¡Œ"):
        df_prev = pd.concat(
            (pd.read_csv(p, nrows=5, encoding=_detect_enc(p)) for p in parts),
            ignore_index=True).head(5)
        st.dataframe(df_prev)

    # ---------- åˆå¹¶ + è¿è¡Œåç«¯ ----------
    if st.button("ğŸš€ Merge & Run backend"):
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")

        # å¤‡ä»½åˆ†ç‰‡
        raw_dir = RAW_ROOT / ts / "parts"
        raw_dir.mkdir(parents=True, exist_ok=True)
        for p in parts: shutil.copy2(p, raw_dir / p.name)
        st.write(f"ğŸ“¦ åˆ†ç‰‡å¤‡ä»½ â†’ {raw_dir}")

        FINAL_PATH.parent.mkdir(parents=True, exist_ok=True)
        merged = _merge(parts, FINAL_PATH)
        st.success(f"âœ… åˆå¹¶å®Œæˆ â†’ {merged}")

        # æ¸…ç©ºæ—§ç›‘æ§æ–‡ä»¶ï¼Œå†™å…¥åˆå§‹çŠ¶æ€
        PROGRESS.write_text(json.dumps({"stage": "starting", "ts": time.time()}))
        LOG_PATH.write_text("")

        # ---------- è°ƒåå°è„šæœ¬å¹¶å®æ—¶åˆ·æ–° ----------
        try:
            with FileLock(str(LOCK_PATH), timeout=1):
                stage_bar = st.progress(0.0)
                sub_bar   = st.progress(0.0)
                stage_txt = st.empty()
                sub_txt   = st.empty()

                t0 = time.perf_counter()
                grids_done = total_grids = 0
                step_idx   = 0
                log_bad: list[str] = []

                proc = subprocess.Popen(
                    BACKEND_CMD,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True,
                    encoding="utf-8",
                    errors="replace",
                    bufsize=1,
                    env={**os.environ, "PYTHONIOENCODING": "utf-8"},
                )

                # æŠŠ PID å†™å…¥æ–‡ä»¶ï¼Œä¾›å‰ç«¯ç»ˆæ­¢
                PID_PATH.write_text(str(proc.pid))

                with open(LOG_PATH, "a", encoding="utf-8") as log_f:
                    for line in proc.stdout:
                        log_f.write(line)
                        log_f.flush()  # ç«‹å³å†™å…¥æ—¥å¿—ï¼Œä¿è¯å®æ—¶æ›´æ–°
                        if _log_re.search(line):
                            log_bad.append(line.rstrip())

                        if m := re.search(r"ï¼{3,}\s*(Step-\d)", line):
                            step_idx += 1
                            stage_txt.text(f"âš™ï¸ {m.group(1)} â€¦")
                            stage_bar.progress(min(step_idx / 5, 1.0))
                            continue

                        if "Step-5" in line and "grids" in line:
                            m2 = re.search(r"(\d+)\s+grids", line)
                            if m2:
                                total_grids = int(m2.group(1))
                            continue

                        if m3 := re.match(r"\[\s*(\d+)/(\d+)]", line):
                            grids_done = int(m3.group(1))
                            total_grids = int(m3.group(2))
                            elapsed = time.perf_counter() - t0
                            eta = elapsed / max(grids_done, 1) * (total_grids - grids_done)
                            sub_txt.text(f"ğŸ“ˆ Sampling {grids_done}/{total_grids} "
                                         f"(ETA â‰ˆ {int(eta//60)} m {int(eta%60)} s)")
                            sub_bar.progress(grids_done / total_grids)
                            continue

                rc = proc.wait()
                stage_bar.progress(1.0)
                sub_bar.progress(1.0)

                if log_bad:
                    st.error("âš ï¸ å‘ç°å…³é”®å‘Šè­¦è¡Œï¼š")
                    st.code("\n".join(log_bad[-200:]), language="bash")
                else:
                    st.success("æµç¨‹å…¨éƒ¨ OKï¼Œè¯¦ç»†æ—¥å¿—è§ consoleã€‚")

                # æ¸…ç† PID
                cleanup_pid()

                if rc == 0:
                    from streamlit.runtime.caching import cache_data
                    cache_data.clear()
                    PROGRESS.write_text(json.dumps({"stage": "FINISHED", "ts": time.time()}))
                    # è¿™é‡Œä¸ç›´æ¥è°ƒç”¨ st.rerun(), ç”±ç”¨æˆ·æ‰‹åŠ¨åˆ·æ–°é¡µé¢
                else:
                    PROGRESS.write_text(json.dumps({"stage": "FAILED", "ts": time.time()}))
                    st.error("åç«¯è„šæœ¬è¿”å›éé›¶ï¼Œè¯¦è§ä¸Šæ–¹å‘Šè­¦ã€‚")

        except Timeout:
            st.warning("âš ï¸ åç«¯æ­£è¢«å…¶ä»–ä»»åŠ¡å ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚")

st.caption("Uploader v1.9.1 Â© CDL")
